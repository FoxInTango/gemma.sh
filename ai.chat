#!/usr/bin/bash


URL=http://192.168.1.128:8081/
API_CHAT=completion
API_V1=v1/chat/completions

# Gemma is GOOD.
content="$1"
system_message="\"parameters\": { \"max_tokens\": 150,\"temperature\": 0.7,\"top_p\": 1,\"system_message\": \"YAML是表达内容结构层级的好方式\"}"
PROMPT=""
PROMPT=${PROMPT}"{\"n_predict\":256,\"messages\":["
PROMPT=${PROMPT}"{\"role\":\"system\",\"content\":\"简单而准确的回答，就是最好的回答.\"},"
PROMPT=${PROMPT}"{\"role\":\"user\",\"content\":\"${content}\",${system_message}}"
PROMPT=${PROMPT}"]"
PROMPT=${PROMPT}"}"
#PROMPT=${PROMPT}"把下面\`\`\` \`\`\`包含的内容做唯一分类映射，把内容类型映射到[哲学 密码学 编程 数学 统计学]中。并用形如{content_id:id,class:classification}的纯粹JSON数据表达映射结果"
#PROMPT=${PROMPT}" \`\`\`content_id : 1001 ,content : $1 \`\`\` "

answer=$(curl -s --request POST                                               \
     --url     ${URL}${API_V1}                                    \
     --header  "Content-Type: application/json"                   \
     -H        "Authorization: Bearer no-key"                     \
     --data    "${PROMPT}")     
echo "回答 ："$answer
#,"{\"n_predict\": 128}"
#--data    "{\"prompt\":\"${PROMPT}\",\"n_predict\": 128}"

echo "\n"

# ./ai.node "把下面 里的内容做唯一分类映射，把内容类型映射到[“哲学” “密码学” “编程” “数学” “统计学”，“Linux”]中。并用形如{"content_id":"id","class":"classification"}的纯粹JSON数据表达映射结果。如果它不符合任何分类，就给出它的关键词。content_id : 1001 ,content : Phi，让我们研究一下 HTML + JavaScript的 markdown渲染吧？"./ai.node "把下面 里的内容做唯一分类映射，把内容类型映射到[“哲学” “密码学” “编程” “数学” “统计学”，“Linux”]中。并用形如{"content_id":"id","class":"classification"}的纯粹JSON数据表达映射结果。如果它不符合任何分类，就给出它的关键词。content_id : 1001 ,content : Phi，让我们研究一下 HTML + JavaScript的 markdown渲染吧？"
#
<< EOF
添加参数以调整模型的表现将取决于您与LLM（如llama-server）交互时使用的具体API。大多数LLMs提供一组可调整的参数以控制生成的输出，例如 temperature、max_tokens、top_p 等。以下是一些常见参数及其描述：

temperature：调整输出的多样性。较高的值会导致更有创造力的输出，而较低的值会导致更确定和预测性的输出。

max_tokens：指定生成的最大输出长度。

top_p（也称为 nucleus sampling）：使用无偏随机采样的顶端p分布选择 token。

relevance 或 presence_penalty：在某些模型中，可用以控制生成内容的相关性或避免重复。

frequency_penalty：用于减少模型生成中频率较高词语的可能性。

presence_penalty：用于增加生成中新颖或不在对话中出现的词语的可能性。

以下是如何在一个 JSON 对象中包含这些参数的示例：

📋 Copy
{
  "messages": [
    {
      "role": "system",
      "content": "你的名字叫llama，你是充满智慧的灵。"
    },
    {
      "role": "user",
      "content": "描述太阳系的历史。",
      "parameters": {
        "max_tokens": 100,
        "temperature": 0.7,
        "top_p": 0.9
      }
    }
  ]
}
在这个示例中，parameters 子集仅适用于 content 对象中的 user 角色。请注意，您需要检查LLM所提供的API文档以了解可用参数、它们的上下文以及如何正确地配置它们。API文档会提供必要的帮助，以便您能够根据您的需求调整模型的表现。
EOF
